{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install shap\n",
    "!pip3 install mosestokenizer\n",
    "!pip3 install truecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('./xmover')\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "import shap\n",
    "from mosestokenizer import MosesDetokenizer, MosesTokenizer\n",
    "from scorer import XMOVERScorer\n",
    "import torch\n",
    "import truecase\n",
    "from xmover_explainer import ExplainableXMover\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FNAME = 'results.json'\n",
    "SRC_LANG = 'ro'\n",
    "TGT_LANG = 'en'\n",
    "SPLIT = 'dev'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f'../../data/{SPLIT}/{SRC_LANG}-{TGT_LANG}-{SPLIT}'\n",
    "src = [s.strip() for s in open(f'{data_dir}/{SPLIT}.src').readlines()]\n",
    "tgt = [s.strip() for s in open(f'{data_dir}/{SPLIT}.mt').readlines()]\n",
    "wor = [list(map(int, s.strip().split())) for s in open(f'{data_dir}/{SPLIT}.tgt-tags').readlines()]\n",
    "sen = [float(s.strip()) for s in open(f'{data_dir}/{SPLIT}.da').readlines()]\n",
    "assert len(src) == len(tgt) == len(wor) == len(sen)\n",
    "dataset = {'src': src, 'tgt': tgt, 'word_labels': wor, 'sent_labels': sen}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get XMover Explainer to Rate and Explain\n",
    "This step can cost quite some time: on a 6-core workstation with a single RTX 2080 GPU card, explaining each translation costs around 3 seconds on average. Hence, explaining all 1000 cases in the dev set takes around 1 hour to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e48d3a50b94349b4b65fbf8f54e695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Permutation explainer: 2it [00:10, 10.36s/it]               \u001b[A\n",
      "\n",
      "Permutation explainer: 2it [00:16, 16.97s/it]               \u001b[A\n",
      "\n",
      "Permutation explainer: 2it [00:16, 16.65s/it]               \u001b[A\n",
      "\n",
      "Permutation explainer: 2it [00:11, 11.49s/it]               \u001b[A\n",
      "\n",
      "Permutation explainer: 2it [00:10, 10.36s/it]               \u001b[A\n",
      "\n",
      "Permutation explainer: 2it [00:10, 10.55s/it]               \u001b[A\n"
     ]
    }
   ],
   "source": [
    "model = ExplainableXMover(SRC_LANG, TGT_LANG)\n",
    "\n",
    "exps = []\n",
    "for i in tqdm(range(len(dataset['src']))):\n",
    "    # score = model(src, trans) # uncomment this line if you also want the xmover-score\n",
    "    exp = model.explain(dataset['src'][i], dataset['tgt'][i])\n",
    "    exps.append(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: save the explanations\n",
    "import pickle\n",
    "with open('{}-{}_exps.pkl'.format(SRC_LANG, TGT_LANG),'wb') as ff:\n",
    "    pickle.dump(exps, ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Quality of the Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have saved some explanations, you can load them\n",
    "import pickle\n",
    "exps = pickle.load(open('{}-{}_exps.pkl'.format(SRC_LANG, TGT_LANG),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scores = []\n",
    "for exp in exps:\n",
    "    scores = [-entry[1] for entry in exp] # use negative SHAP values to find the incorrect tokens\n",
    "    exp_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 0.638\n",
      "AP score: 0.464\n",
      "Recall at top-K: 0.339\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('../..')\n",
    "from scripts.evaluate import evaluate_word_level\n",
    "\n",
    "evaluate_word_level(dataset['word_labels'], exp_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_explain",
   "language": "python",
   "name": "venv_explain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
